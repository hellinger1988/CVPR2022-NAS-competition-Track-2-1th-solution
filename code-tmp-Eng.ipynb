{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T14:15:08.514511Z",
     "iopub.status.busy": "2022-05-21T14:15:08.513276Z",
     "iopub.status.idle": "2022-05-21T14:15:21.950058Z",
     "shell.execute_reply": "2022-05-21T14:15:21.949373Z",
     "shell.execute_reply.started": "2022-05-21T14:15:08.514462Z"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Our 2022 CVPR Track2 Solution \n",
    "\n",
    "Accurately Predicting the performance of architecture with small sample training is a very important but not easy task. How to analysis and train data as much as we can and overcome over fitting is the core problem we should deal with. Meanwhile if there is the mult-task problem, we should also think about if we can take advantage of their correlation. \n",
    "\n",
    "In this track Super Network builds a search space based on ViT-Base.The search space contain depth, num_heads, mpl_ratio and embed_dim. \n",
    "\n",
    "### Pre process the data\n",
    "Simplest way is directly train all the network after transform the structure to float and predict. The problem of this way is singularity and does not take advantage of information of background information. Therefore, we pre process the data by following step.\n",
    "> - Transfer the depth encoding(j,k,l) to integer(1,2,3). Here we consider ordinal encoding instead of one-Hot encoding as we assume depth has some correlation with predictability, the experiment also confirm our thought.\n",
    "\n",
    "> - If the actual depth of a sub-network is less than 12, its encoding trailing end is padded with 0. Here we change 0 to 2 as we assume the input is 1,2,3 and 2 represent neutral information. after this transformation, the correlation between depth encoding feature and others decreased a lot which is a good thing for fitting later. Then we normalize data that map (1,2,3) to (-1,0,1).\n",
    "\n",
    "> - Using Sigmoid function as Activiation Function. As we known, rank follow uniform distribution. By Sigmoid function we transfer uniform distribution to Gaussian distribution which is best choice for most model. After we get prediction, we use Sigmoid function again to transfer output back to uniform distribution as rank, then we round up to nearest integer. \n",
    "\n",
    "> - we also try lots of other ideas such as different activiation function, add gassian noise. None of them has obviously improve across different task.\n",
    "\n",
    "### Model selection\n",
    "\n",
    "What we did and how we pick our model.\n",
    "\n",
    "> - Only GPNAS model, the baseline or modified based on baseline, the final score is about 0.67.\n",
    "\n",
    "> - Other machine learning model such as linear regression, random forest model, kernel ridge regression, tree model, XGBoost, Gradient Boosting and so on. We find Gradient Boosting or other boosting based algorithm have best average result. When we firstly try it, we get score about 0.73. after tunning parameters, we get 0.78. \n",
    "\n",
    "> - As our problem is multi-task, could we train different task at same time? We consider Multivariate Gradient Boosting based on muliMSE. However, the result is worse than univariate gradient boosting method. We think the reason is different task should consider different hyperparameters and we can not reduce the noise in Multivariate Gradient Boosting effectively.\n",
    "\n",
    "> - As our trainig sample size is very small, in order to avoid over fitting, we try to ensemble our models. We choose GBRT,HISTGB,CATGB,XGB as our sub model. The obvious way is average all the result, this result improve to about 0.79. Then we try to improve after naive averaging. \n",
    "\n",
    "> - We tried bagging Gradient Boosting which doesn't improve result. Then we think about stack gradient boosting group of model by a final estimator and the final estimator we use is GPNAS.\n",
    "Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator. As sklearn already provide us this regressor, we modified GPNAS as a sklearn API. Now we get result about 0.792.\n",
    "\n",
    "> - Until now our submodel of stack model still use same hyperparameters such as loss function, learning rate and depth. Then we consider select different loss function, learning rate and depth for different model as we assume different submodel and task have total different feature. And we add two more GBRT,CATGB submodel with different loss funtion. This round we get result about 0.798.\n",
    "\n",
    "> - Also we modified and tunning our final estmator GPNAS, such as ridge parameter and instead of identity matrix, we choose inv(X.T*X) as our prior covariance matrix. We get final score 0.7991.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:20:11.720442Z",
     "iopub.status.busy": "2022-05-26T05:20:11.719968Z",
     "iopub.status.idle": "2022-05-26T05:24:59.094690Z",
     "shell.execute_reply": "2022-05-26T05:24:59.094010Z",
     "shell.execute_reply.started": "2022-05-26T05:20:11.720401Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting catboost\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/77/6b/8b6c6e303208d36725f9544ff0659c6085cd737bb6bdfbbcb2bc9e34c290/catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m308.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (1.6.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (1.1.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (1.20.3)\n",
      "Collecting plotly\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/79/92/f8282ed209c2e7d4077c5f236bc241d42fb9a2b2986330188909a60be7bb/plotly-5.8.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m342.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: graphviz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (0.13)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas>=0.24.0->catboost) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->catboost) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/f2/a5/f86bc8d67c979020438c8559cc70cfe3a1643fd160d35e09c9cca6a09189/tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (56.2.0)\n",
      "Installing collected packages: tenacity, plotly, catboost\n",
      "Successfully installed catboost-1.0.6 plotly-5.8.0 tenacity-8.0.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m0.24.2\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Install Packages\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "#!mkdir /home/aistudio/external-libraries\n",
    "#!pip install --upgrade sklearn -i https://mirrors.aliyun.com/pypi/simple/ -t /home/aistudio/externallibraries\n",
    "#!conda install lightgbm \n",
    "#!conda install xgboost -i https://mirrors.aliyun.com/pypi/simple/\n",
    "!pip install catboost -i https://mirrors.aliyun.com/pypi/simple/\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Import Packages\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import catboost\n",
    "import lightgbm\n",
    "import xgboost\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import *#HistGradientBoostingRegressor,StackingRegressor,BaggingRegressor,ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import *\n",
    "from sklearn.linear_model import *#LinearRegression\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import scipy\n",
    "import copy\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.linalg import hankel\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:34:36.726081Z",
     "iopub.status.busy": "2022-05-26T05:34:36.725492Z",
     "iopub.status.idle": "2022-05-26T05:34:39.223755Z",
     "shell.execute_reply": "2022-05-26T05:34:39.222980Z",
     "shell.execute_reply.started": "2022-05-26T05:34:36.726037Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Load trainning and Testing data\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "def convert_X(arch_str):\n",
    "    temp_arch = []\n",
    "    for i,elm in enumerate(arch_str):\n",
    "        if i in [3,6,9,12,15,18,21,24,27,30,33,36]: pass #Get rid of non-info columns,all is 768\n",
    "        elif elm == 'j': temp_arch.append(1-2) #Transform it to number then central data, normalize Data\n",
    "        elif elm == 'k': temp_arch.append(2-2) #Transform it to number then central data, normalize Data\n",
    "        elif elm == 'l': temp_arch.append(3-2)  #Transform it to number then central data, normalize Data\n",
    "        elif int(elm) == 0: temp_arch.append(2-2)  #Make 0 as 2 as it should contain neutral information（reduce correlation), then central data, normalize Data\n",
    "        else: temp_arch.append(int(elm)-2)  #central data,normalize Data\n",
    "    return(temp_arch)\n",
    "with open('./data/data134077/CVPR_2022_NAS_Track2_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('./data/data134077/CVPR_2022_NAS_Track2_test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "    \n",
    "test_arch_list = []\n",
    "for key in test_data.keys():\n",
    "    test_arch =  convert_X(test_data[key]['arch'])\n",
    "    test_arch_list.append(test_arch)\n",
    "bb = np.array(test_arch_list).T\n",
    "\n",
    "train_list = [[],[],[],[],[],[],[],[]]\n",
    "arch_list_train = []\n",
    "name_list = ['cplfw_rank', 'market1501_rank', 'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank', 'veriwild_rank', 'sop_rank']\n",
    "for key in train_data.keys():\n",
    "    for idx, name in enumerate(name_list):\n",
    "        train_list[idx].append(train_data[key][name])\n",
    "    xx = train_data[key]['arch']\n",
    "    arch_list_train.append(convert_X(train_data[key]['arch']))\n",
    "\n",
    "Y_all0 = np.array(train_list)\n",
    "Y_all = np.log((Y_all0+1)/(500-Y_all0)) #Transfer rank data by Sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:34:39.237571Z",
     "iopub.status.busy": "2022-05-26T05:34:39.237235Z",
     "iopub.status.idle": "2022-05-26T05:34:39.349132Z",
     "shell.execute_reply": "2022-05-26T05:34:39.348498Z",
     "shell.execute_reply.started": "2022-05-26T05:34:39.237548Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Corr: \n",
      "[[ 1.    0.18  0.12 -0.17 -0.29 -0.3  -0.3  -0.13]\n",
      " [ 0.18  1.    0.78  0.81  0.52  0.3   0.53  0.4 ]\n",
      " [ 0.12  0.78  1.    0.85  0.62  0.2   0.57  0.52]\n",
      " [-0.17  0.81  0.85  1.    0.8   0.42  0.82  0.66]\n",
      " [-0.29  0.52  0.62  0.8   1.    0.46  0.77  0.48]\n",
      " [-0.3   0.3   0.2   0.42  0.46  1.    0.52  0.12]\n",
      " [-0.3   0.53  0.57  0.82  0.77  0.52  1.    0.69]\n",
      " [-0.13  0.4   0.52  0.66  0.48  0.12  0.69  1.  ]]\n",
      "Parameters Corr: \n",
      "[[ 1.    0.   -0.02  0.02 -0.09 -0.01  0.03 -0.02  0.09  0.01  0.   -0.05\n",
      "   0.1  -0.04 -0.05 -0.02  0.04  0.01  0.   -0.08 -0.03  0.05  0.07  0.06\n",
      "  -0.11]\n",
      " [ 0.    1.    0.01 -0.05  0.05  0.12  0.    0.02 -0.01 -0.02  0.01  0.1\n",
      "   0.05 -0.03 -0.01  0.    0.03 -0.09 -0.01 -0.07  0.02  0.02  0.03  0.05\n",
      "  -0.07]\n",
      " [-0.02  0.01  1.    0.01  0.03  0.01  0.1  -0.06  0.05  0.04  0.01  0.\n",
      "  -0.01 -0.03 -0.01 -0.08 -0.01  0.03  0.05  0.04  0.01  0.03 -0.01  0.05\n",
      "   0.03]\n",
      " [ 0.02 -0.05  0.01  1.    0.05 -0.03  0.01  0.   -0.02 -0.03 -0.08  0.08\n",
      "  -0.04 -0.01  0.04 -0.01  0.04  0.03  0.01 -0.03 -0.02 -0.01  0.04  0.05\n",
      "   0.09]\n",
      " [-0.09  0.05  0.03  0.05  1.   -0.02  0.04 -0.06  0.05  0.01 -0.03  0.\n",
      "  -0.04 -0.03 -0.04  0.01  0.03 -0.03  0.07  0.08  0.04  0.03  0.03  0.\n",
      "  -0.03]\n",
      " [-0.01  0.12  0.01 -0.03 -0.02  1.    0.02 -0.04 -0.04 -0.02 -0.06 -0.\n",
      "   0.01  0.05  0.01 -0.09  0.   -0.05 -0.08 -0.04  0.01  0.08  0.07 -0.04\n",
      "  -0.03]\n",
      " [ 0.03  0.    0.1   0.01  0.04  0.02  1.   -0.03  0.01 -0.02 -0.02  0.01\n",
      "  -0.05 -0.04 -0.03  0.02 -0.04 -0.06 -0.03  0.04 -0.03  0.01 -0.02  0.\n",
      "   0.01]\n",
      " [-0.02  0.02 -0.06  0.   -0.06 -0.04 -0.03  1.    0.01  0.04 -0.01 -0.09\n",
      "   0.07  0.07 -0.01  0.05 -0.    0.05  0.05 -0.11 -0.02 -0.04 -0.   -0.04\n",
      "   0.02]\n",
      " [ 0.09 -0.01  0.05 -0.02  0.05 -0.04  0.01  0.01  1.   -0.06 -0.03 -0.04\n",
      "   0.06  0.05 -0.09  0.   -0.01  0.01  0.03  0.08 -0.01 -0.03  0.1   0.02\n",
      "   0.08]\n",
      " [ 0.01 -0.02  0.04 -0.03  0.01 -0.02 -0.02  0.04 -0.06  1.   -0.05 -0.06\n",
      "   0.02 -0.06 -0.06  0.   -0.04 -0.13 -0.03 -0.06 -0.03 -0.01 -0.02  0.03\n",
      "  -0.06]\n",
      " [ 0.    0.01  0.01 -0.08 -0.03 -0.06 -0.02 -0.01 -0.03 -0.05  1.   -0.04\n",
      "  -0.06  0.   -0.02 -0.06  0.03 -0.04  0.07 -0.09  0.02 -0.01 -0.04  0.01\n",
      "  -0.02]\n",
      " [-0.05  0.1   0.    0.08  0.   -0.    0.01 -0.09 -0.04 -0.06 -0.04  1.\n",
      "   0.02  0.02  0.09 -0.02 -0.04  0.02  0.    0.02  0.03 -0.03  0.03 -0.04\n",
      "  -0.03]\n",
      " [ 0.1   0.05 -0.01 -0.04 -0.04  0.01 -0.05  0.07  0.06  0.02 -0.06  0.02\n",
      "   1.   -0.07 -0.01  0.09 -0.06  0.06  0.03  0.01 -0.   -0.03 -0.06 -0.06\n",
      "   0.02]\n",
      " [-0.04 -0.03 -0.03 -0.01 -0.03  0.05 -0.04  0.07  0.05 -0.06  0.    0.02\n",
      "  -0.07  1.    0.02  0.04  0.    0.05 -0.   -0.04 -0.06 -0.05  0.06 -0.05\n",
      "  -0.07]\n",
      " [-0.05 -0.01 -0.01  0.04 -0.04  0.01 -0.03 -0.01 -0.09 -0.06 -0.02  0.09\n",
      "  -0.01  0.02  1.    0.02  0.03  0.03  0.02 -0.04  0.03  0.01 -0.05 -0.05\n",
      "  -0.01]\n",
      " [-0.02  0.   -0.08 -0.01  0.01 -0.09  0.02  0.05  0.    0.   -0.06 -0.02\n",
      "   0.09  0.04  0.02  1.   -0.05  0.05  0.04 -0.04 -0.02 -0.04  0.01  0.03\n",
      "  -0.03]\n",
      " [ 0.04  0.03 -0.01  0.04  0.03  0.   -0.04 -0.   -0.01 -0.04  0.03 -0.04\n",
      "  -0.06  0.    0.03 -0.05  1.    0.06 -0.   -0.04  0.04 -0.02  0.03  0.02\n",
      "   0.08]\n",
      " [ 0.01 -0.09  0.03  0.03 -0.03 -0.05 -0.06  0.05  0.01 -0.13 -0.04  0.02\n",
      "   0.06  0.05  0.03  0.05  0.06  1.    0.    0.04 -0.06 -0.02  0.05 -0.\n",
      "  -0.06]\n",
      " [ 0.   -0.01  0.05  0.01  0.07 -0.08 -0.03  0.05  0.03 -0.03  0.07  0.\n",
      "   0.03 -0.    0.02  0.04 -0.    0.    1.   -0.03 -0.02  0.03 -0.03  0.03\n",
      "  -0.08]\n",
      " [-0.08 -0.07  0.04 -0.03  0.08 -0.04  0.04 -0.11  0.08 -0.06 -0.09  0.02\n",
      "   0.01 -0.04 -0.04 -0.04 -0.04  0.04 -0.03  1.   -0.    0.07 -0.05 -0.1\n",
      "  -0.  ]\n",
      " [-0.03  0.02  0.01 -0.02  0.04  0.01 -0.03 -0.02 -0.01 -0.03  0.02  0.03\n",
      "  -0.   -0.06  0.03 -0.02  0.04 -0.06 -0.02 -0.    1.    0.02  0.03 -0.03\n",
      "   0.05]\n",
      " [ 0.05  0.02  0.03 -0.01  0.03  0.08  0.01 -0.04 -0.03 -0.01 -0.01 -0.03\n",
      "  -0.03 -0.05  0.01 -0.04 -0.02 -0.02  0.03  0.07  0.02  1.   -0.1   0.05\n",
      "  -0.11]\n",
      " [ 0.07  0.03 -0.01  0.04  0.03  0.07 -0.02 -0.    0.1  -0.02 -0.04  0.03\n",
      "  -0.06  0.06 -0.05  0.01  0.03  0.05 -0.03 -0.05  0.03 -0.1   1.   -0.04\n",
      "  -0.05]\n",
      " [ 0.06  0.05  0.05  0.05  0.   -0.04  0.   -0.04  0.02  0.03  0.01 -0.04\n",
      "  -0.06 -0.05 -0.05  0.03  0.02 -0.    0.03 -0.1  -0.03  0.05 -0.04  1.\n",
      "   0.05]\n",
      " [-0.11 -0.07  0.03  0.09 -0.03 -0.03  0.01  0.02  0.08 -0.06 -0.02 -0.03\n",
      "   0.02 -0.07 -0.01 -0.03  0.08 -0.06 -0.08 -0.    0.05 -0.11 -0.05  0.05\n",
      "   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#Correlation reserach \n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "regcoef = np.round(np.array([LinearRegression().fit(np.array(arch_list_train), Y_all[i]).coef_ for i in range(8)]),2)\n",
    "print('Task Corr: ')\n",
    "print(np.round(np.corrcoef(regcoef),2))#correlation bewtween different task\n",
    "print('Parameters Corr: ')\n",
    "print(np.round(np.corrcoef(np.array(arch_list_train).T.astype('float')),2))\n",
    "#correlation bewtween different task, we can find the correlation is very low between different arch parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:17:40.796976Z",
     "iopub.status.busy": "2022-05-24T00:17:40.796710Z",
     "iopub.status.idle": "2022-05-24T00:17:40.822977Z",
     "shell.execute_reply": "2022-05-24T00:17:40.822423Z",
     "shell.execute_reply.started": "2022-05-24T00:17:40.796950Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Modify GPNAS code as sklearn API and add some more function\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "__all__ = [\"GPNAS_API\"]\n",
    "\n",
    "class GPNAS_API(object):\n",
    "    _estimator_type = \"regressor\"\n",
    "    def __init__(self, cov_w = None, w = None, c_flag=2, m_flag=2, hp_mat = 0.0000001, hp_cov = 0.01, icov = 1):\n",
    "        self.hp_mat = hp_mat\n",
    "        self.hp_cov = hp_cov\n",
    "        self.cov_w = cov_w\n",
    "        self.w = w \n",
    "        self.c_flag = c_flag\n",
    "        self.m_flag = m_flag\n",
    "        self.icov = icov #if we use initial cov as prior\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"hp_mat\": self.hp_mat, \n",
    "            \"hp_cov\": self.hp_cov, \n",
    "            \"cov_w\": self.cov_w, \n",
    "            \"w\": self.w, \n",
    "            \"c_flag\": self.c_flag, \n",
    "            \"m_flag\": self.m_flag, \n",
    "            \"icov\": self.icov, \n",
    "            }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def _get_corelation(self, mat1, mat2):\n",
    "        \"\"\"\n",
    "        give two typical kernel function\n",
    "        Auto kernel hyperparameters estimation to be updated\n",
    "        \"\"\"\n",
    "\n",
    "        mat_diff = abs(mat1 - mat2)\n",
    "\n",
    "        if self.c_flag == 1:\n",
    "\n",
    "            return 0.5 * np.exp(-np.dot(mat_diff, mat_diff) / 16)\n",
    "\n",
    "        elif self.c_flag == 2:\n",
    "\n",
    "            return 1 * np.exp(-np.sqrt(np.dot(mat_diff, mat_diff)) / 12)\n",
    "\n",
    "    def _preprocess_X(self, X):\n",
    "        \"\"\"\n",
    "        preprocess of input feature/ tokens of architecture\n",
    "        more complicated preprocess can be added such as nonlineaer transformation\n",
    "        \"\"\"\n",
    "        X = X.tolist()\n",
    "        p_X = copy.deepcopy(X)\n",
    "        for feature in p_X: feature.append(1)\n",
    "        return p_X\n",
    "\n",
    "    def _get_cor_mat(self, X):\n",
    "        \"\"\"get kernel matrix\"\"\"\n",
    "        X = np.array(X)\n",
    "        l = X.shape[0]\n",
    "        cor_mat = []\n",
    "\n",
    "        for c_idx in range(l):\n",
    "            col = []\n",
    "            c_mat = X[c_idx].copy()\n",
    "\n",
    "            for r_idx in range(l):\n",
    "                r_mat = X[r_idx].copy()\n",
    "                temp_cor = self._get_corelation(c_mat, r_mat)\n",
    "                col.append(temp_cor)\n",
    "            cor_mat.append(col)\n",
    "\n",
    "        return np.mat(cor_mat)\n",
    "\n",
    "    def _get_cor_mat_joint(self, X, X_train):\n",
    "        \"\"\"\n",
    "        get kernel matrix\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        X_train = np.array(X_train)\n",
    "        l_c = X.shape[0]\n",
    "        l_r = X_train.shape[0]\n",
    "        cor_mat = []\n",
    "\n",
    "        for c_idx in range(l_c):\n",
    "            col = []\n",
    "            c_mat = X[c_idx].copy()\n",
    "\n",
    "            for r_idx in range(l_r):\n",
    "                r_mat = X_train[r_idx].copy()\n",
    "                temp_cor = self._get_corelation(c_mat, r_mat)\n",
    "                col.append(temp_cor)\n",
    "            cor_mat.append(col)\n",
    "\n",
    "        return np.mat(cor_mat)\n",
    "\n",
    "\n",
    "    def fit(self, X,y):\n",
    "        self.get_initial_mean(X[0::2],y[0::2])\n",
    "        self.get_initial_cov(X)\n",
    "        # 更新（训练）gpnas预测器超参数\n",
    "        self.get_posterior_mean(X[1::2],y[1::2])\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        #print('beta',self.w.flatten())\n",
    "        return X * self.w\n",
    "            \n",
    "    def get_predict(self, X):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "\n",
    "        return X * self.w\n",
    "\n",
    "    def get_predict_jiont(self, X, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X based on X_train and Y_train\n",
    "        \"\"\"\n",
    "        X = np.mat(X)\n",
    "        X_train = np.mat(X_train)\n",
    "        Y_train = np.mat(Y_train)\n",
    "        m_X = self.get_predict(X)\n",
    "        m_X_train = self.get_predict(X_train)\n",
    "        mat_train = self._get_cor_mat(X_train)\n",
    "        mat_joint = self._get_cor_mat_joint(X, X_train)\n",
    "\n",
    "        return m_X + mat_joint * np.linalg.inv(mat_train + self.hp_mat * np.eye(\n",
    "            X_train.shape[0])) * (Y_train.T - m_X_train)\n",
    "\n",
    "    def get_initial_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get initial mean of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        self.w = np.linalg.inv(X.T * X + self.hp_mat * np.eye(X.shape[\n",
    "            1])) * X.T * Y.T\n",
    "        #inv(X.T*X)X.T*Y as initial mean\n",
    "        print('Variance',np.var(Y-X * self.w))#Show variance of residual then we can base this tunning self.hp_cov\n",
    "        return self.w\n",
    "\n",
    "    def get_initial_cov(self, X):\n",
    "        \"\"\"\n",
    "        get initial coviarnce matrix of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        if self.icov == 1: #use inv(X.T*X) as initial covariance\n",
    "            self.cov_w = self.hp_cov * np.linalg.inv(X.T * X)\n",
    "        elif self.icov == 0:# use identity matrix as initial covariance\n",
    "            self.cov_w = self.hp_cov * np.eye(X.shape[1])\n",
    "        else:\n",
    "            assert 0,'not available yet'\n",
    "        return self.cov_w\n",
    "\n",
    "    def get_posterior_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior mean of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        cov_mat = self._get_cor_mat(X)\n",
    "        if self.m_flag == 1:\n",
    "            self.w = self.w + self.cov_w * X.T * np.linalg.inv(\n",
    "                np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0])) + X *\n",
    "                self.cov_w * X.T + self.hp_mat * np.eye(X.shape[0])) * (\n",
    "                    Y.T - X * self.w)\n",
    "        else:\n",
    "            self.w = np.linalg.inv(X.T * np.linalg.inv(\n",
    "                cov_mat + self.hp_mat * np.eye(X.shape[0])) * X + np.linalg.inv(\n",
    "                    self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                        1])) + self.hp_mat * np.eye(X.shape[1])) * (\n",
    "                            X.T * np.linalg.inv(cov_mat + self.hp_mat * np.eye(\n",
    "                                X.shape[0])) * Y.T +\n",
    "                            np.linalg.inv(self.cov_w + self.hp_mat * np.eye(\n",
    "                                X.shape[1])) * self.w)\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def get_posterior_cov(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior coviarnce matrix of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        cov_mat = self._get_cor_mat(X)\n",
    "        self.cov_mat = np.linalg.inv(\n",
    "            np.linalg.inv(X.T * cov_mat * X + self.hp_mat * np.eye(X.shape[1]))\n",
    "            + np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                1])) + self.hp_mat * np.eye(X.shape[1]))\n",
    "        return self.cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:17:42.370303Z",
     "iopub.status.busy": "2022-05-24T00:17:42.370035Z",
     "iopub.status.idle": "2022-05-24T00:17:42.384130Z",
     "shell.execute_reply": "2022-05-24T00:17:42.383589Z",
     "shell.execute_reply.started": "2022-05-24T00:17:42.370278Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Modify GPNAS code as sklearn API and add some more function\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "max_iter = [10000,10000,10000,10000,10000,10000,10000,10000] \n",
    "\n",
    "#learning_rate = [0.008,0.038,0.032,0.02,0.025,0.012,0.025,0.006]\n",
    "#learning_rate = [0.01.,0.04.,0.04.,0.04.,0.02.,0.02.,0.04.,0.001.]\n",
    "learning_rate = [0.005,0.038,0.035,0.03,0.025,0.01,0.03,0.01] #Final learning rate\n",
    "max_depth = [1,3,2,2,2,3,1,3] #depth for GBRT(huber),CATGB(MSE),GBRT2(MSE),CATGB2(huber)\n",
    "max_depth2 = [1,1,1,1,1,1,1,1] #depth for HISTGB,LIGHTGB,XGB\n",
    "list_est = []\n",
    "\n",
    "model_GBRT,model_HISTGB,model_CATGB,model_LIGHTGB,model_XGB,model_GBRT2,model_CATGB2= [],[],[],[],[],[],[]\n",
    "for i in range(8):\n",
    "\n",
    "    params_GBRT = {\"n_estimators\": max_iter[i],\n",
    "    \"max_depth\": max_depth[i],\n",
    "    \"subsample\": .8,\n",
    "    \"learning_rate\": learning_rate[i],\n",
    "    \"loss\": 'huber',\n",
    "    \"max_features\": 'sqrt',\n",
    "    \"random_state\":1,\n",
    "    } \n",
    "    model_GBRT.append(ensemble.GradientBoostingRegressor(**params_GBRT)) \n",
    "    \n",
    "    params_HISTGB = {\n",
    "    \"max_depth\": max_depth2[i],\n",
    "    \"max_iter\":max_iter[i] ,\n",
    "    \"learning_rate\": learning_rate[i],\n",
    "    \"loss\": 'least_squares',\n",
    "    \"max_leaf_nodes\":31,\n",
    "    \"min_samples_leaf\":5,\n",
    "    \"l2_regularization\":5,\n",
    "    \"random_state\":1,\n",
    "    }\n",
    "    model_HISTGB.append(HistGradientBoostingRegressor(**params_HISTGB))\n",
    "\n",
    "\n",
    "    model_CATGB.append(catboost.CatBoostRegressor(iterations= max_iter[i] ,\n",
    "                             learning_rate= learning_rate[i],\n",
    "                             depth= max_depth[i],\n",
    "                             silent=True,\n",
    "                             task_type=\"CPU\",\n",
    "                             loss_function= 'RMSE',                     \n",
    "                             eval_metric='RMSE',\n",
    "                             random_seed = 1,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 75,\n",
    "                             od_wait=100,\n",
    "                             ))\n",
    "\n",
    "    \n",
    "    model_LIGHTGB.append(lightgbm.LGBMRegressor(boosting_type='gbdt',learning_rate = learning_rate[i],num_leaves=31,\n",
    "    max_depth = max_depth2[i], alpha = 0.1, n_estimators = max_iter[i] ,random_state=1))\n",
    "    \n",
    "    model_XGB.append(xgboost.XGBRegressor(learning_rate = learning_rate[i],tree_method = 'auto',\n",
    "    max_depth = max_depth2[i], alpha = 0.8, n_estimators = max_iter[i] ,random_state=1))   \n",
    "\n",
    "    params_GBRT2 = {\"n_estimators\": max_iter[i] ,\n",
    "    \"max_depth\": max_depth[i],\n",
    "    \"subsample\": .8,\n",
    "    \"learning_rate\": learning_rate[i],\n",
    "    \"loss\": 'ls',\n",
    "    \"max_features\": 'log2',\n",
    "    \"random_state\":1,\n",
    "    } \n",
    "    model_GBRT2.append(ensemble.GradientBoostingRegressor(**params_GBRT2)) \n",
    "    \n",
    "    model_CATGB2.append(catboost.CatBoostRegressor(iterations= max_iter[i] ,\n",
    "                             learning_rate= learning_rate[i],\n",
    "                             depth= max_depth[i],\n",
    "                             silent=True,\n",
    "                             task_type=\"CPU\",\n",
    "                             loss_function=  'Huber:delta=2',                     \n",
    "                             eval_metric= 'Huber:delta=2',\n",
    "                             random_seed = 1,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 75,\n",
    "                             od_wait=100,\n",
    "                             l2_leaf_reg = 1,\n",
    "                             subsample = 0.8,\n",
    "                             ))\n",
    "\n",
    "for i in range(8): \n",
    "    list_est.append([\n",
    "    ('GBRT', model_GBRT[i]),\n",
    "    ('HISTGB', model_HISTGB[i]),\n",
    "    ('CATGB',model_CATGB[i]),\n",
    "    ('LIGHTGB', model_LIGHTGB[i]),\n",
    "    ('XGB', model_XGB[i]),\n",
    "    ('GBRT2', model_GBRT2[i]),\n",
    "    ('CATGB2', model_CATGB2[i]),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:01.070177Z",
     "iopub.status.busy": "2022-05-23T13:13:01.069587Z",
     "iopub.status.idle": "2022-05-23T13:13:01.073095Z",
     "shell.execute_reply": "2022-05-23T13:13:01.072552Z",
     "shell.execute_reply.started": "2022-05-23T13:13:01.070147Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    In sample training and testing, just for reserach and parameter selection\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#X_all_k = np.array(arch_list_train)\n",
    "#X_val = np.array(test_arch_list)\n",
    "#print(np.array(test_arch_list).shape,X_val.shape)\n",
    "#train_num = 400;\n",
    "#gb_list = []\n",
    "#for i in range(8):\n",
    "#    model_final = StackingRegressor(estimators=list_est[i],final_estimator=GPNAS_API(c_flag=2, m_flag=2, hp_mat = hp_list[i], hp_cov = 3, icov = 1),passthrough=False,n_jobs=4)\n",
    "#    Y_all_k = Y_all[i]\n",
    "#    X_train_k, Y_train_k, X_test_k, Y_test_k = X_all_k[0:train_num:1], Y_all_k[0:train_num:1], X_all_k[train_num::1], Y_all_k[train_num::1]\n",
    "#    model_final.fit(X_train_k,Y_train_k)\n",
    "#    gb_list.append(copy.copy(model_final))\n",
    "#    print('Kendalltau:',i,scipy.stats.stats.kendalltau(model_final.predict(X_test_k),Y_test_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:03.412908Z",
     "iopub.status.busy": "2022-05-23T13:13:03.412270Z",
     "iopub.status.idle": "2022-05-23T13:13:03.416007Z",
     "shell.execute_reply": "2022-05-23T13:13:03.415503Z",
     "shell.execute_reply.started": "2022-05-23T13:13:03.412880Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Plot training and testing params result to help make prediction.\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#i = 0\n",
    "#params = gb_list[i].get_params()\n",
    "#itr = \"n_estimators\"\n",
    "#test_score0 = np.zeros((params[itr],), dtype=np.float64)\n",
    "#test_score1 = np.zeros((params[itr],), dtype=np.float64)\n",
    "\n",
    "#for i,y_pred0 in enumerate(model_gb.staged_predict(X_train_k)):\n",
    "#    test_score0[i] = scipy.stats.stats.kendalltau(Y_train_k, y_pred0)[0]\n",
    "#for i,y_pred1 in enumerate(model_gb.staged_predict(X_test_k)):\n",
    "#    test_score1[i] = scipy.stats.stats.kendalltau(Y_test_k, y_pred1)[0]\n",
    "    \n",
    "#fig = plt.figure(figsize=(6, 6))\n",
    "#plt.subplot(1, 1, 1)\n",
    "#plt.title(\"Deviance\")\n",
    "#plt.plot(\n",
    "#    np.arange(params[itr])[:] + 1,\n",
    "#    test_score0[:],\n",
    "#    \"b-\",\n",
    "#    label=\"Training Set Deviance\",\n",
    "#)\n",
    "#plt.plot(np.arange(params[itr])[:] + 1, test_score1[:], \"r-\", label=\"Test Set Deviance\")\n",
    "#plt.legend(loc=\"upper right\")\n",
    "#plt.xlabel(\"Boosting Iterations\")\n",
    "#plt.ylabel(\"Deviance\")\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:05.791280Z",
     "iopub.status.busy": "2022-05-23T13:13:05.790682Z",
     "iopub.status.idle": "2022-05-23T13:13:05.794026Z",
     "shell.execute_reply": "2022-05-23T13:13:05.793500Z",
     "shell.execute_reply.started": "2022-05-23T13:13:05.791254Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Feature Importance\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#from sklearn.inspection import permutation_importance\n",
    "#for model_gb in gb_list:\n",
    "#  feature_importance = model_gb.feature_importances_\n",
    "#  sorted_idx = np.argsort(feature_importance)\n",
    "#  pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "#  fig = plt.figure(figsize=(6, 4))\n",
    "#  plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "#  plt.yticks(pos, np.array(range(len(feature_importance)))[sorted_idx])\n",
    "#  plt.title(\"Feature Importance \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:47.011185Z",
     "iopub.status.busy": "2022-05-23T13:13:47.010571Z",
     "iopub.status.idle": "2022-05-23T14:29:06.242003Z",
     "shell.execute_reply": "2022-05-23T14:29:06.240534Z",
     "shell.execute_reply.started": "2022-05-23T13:13:47.011158Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:  0\n",
      "Variance 3.3439391282630595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165/1501576174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mY_train_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mzz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Transfer by Sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrank_all1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         return self.final_estimator_.predict(\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         )\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mPrediction\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \"\"\"\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sk_visual_block_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    215\u001b[0m         predictions = [\n\u001b[1;32m    216\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_method_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         ]\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_method_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         ]\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concatenate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;31m# Return inverse link of raw predictions after converting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;31m# shape (n_samples, 1) to (n_samples,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_link_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mraw_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_baseline_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         self._predict_iterations(\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_binned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         )\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36m_predict_iterations\u001b[0;34m(self, X, predictors, raw_predictions, is_binned)\u001b[0m\n\u001b[1;32m    771\u001b[0m                         \u001b[0mknown_cat_bitsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknown_cat_bitsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                         f_idx_map=f_idx_map)\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0mraw_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_staged_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/ensemble/_hist_gradient_boosting/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, known_cat_bitsets, f_idx_map)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_DTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         _predict_from_raw_data(self.nodes, X, self.raw_left_cat_bitsets,\n\u001b[0;32m---> 67\u001b[0;31m                                known_cat_bitsets, f_idx_map, out)\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Result 1, use GPNAS with inv(X.T*X) as initial covariance prior to stack 'GBRT','HISTGB','CATGB','LIGHTGB','XGB','GBRT2','CATGB2' with cross validation\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "X_train_k = np.array(arch_list_train)\n",
    "X_val = np.array(test_arch_list)\n",
    "\n",
    "rank_all1= []\n",
    "for i in range(len(list_est)):\n",
    "    print('No: ',i)\n",
    "    #stack different regressor by GPNAS with cross validation\n",
    "    model_final = StackingRegressor(estimators=list_est[i],final_estimator=GPNAS_API(c_flag=2, m_flag=2, hp_mat = 0.5, hp_cov = 3, icov = 1),passthrough=False,n_jobs=4)\n",
    "    Y_train_k = Y_all[i]\n",
    "    model_final.fit(X_train_k,Y_train_k)\n",
    "    zz = np.round((X_val.shape[0]-1)/(1+ np.exp(-1*model_final.predict(X_val)))) #Transfer by Sigmoid function\n",
    "    print(zz[0])\n",
    "    rank_all1.append(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-22T14:29:30.156108Z",
     "iopub.status.idle": "2022-05-22T14:29:30.156419Z",
     "shell.execute_reply": "2022-05-22T14:29:30.156270Z",
     "shell.execute_reply.started": "2022-05-22T14:29:30.156256Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Save Result 1(My result is ran locally which is a little different from here)\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "for idx,key in enumerate(test_data.keys()):\n",
    "    #print(key)\n",
    "    test_data[key]['cplfw_rank'] = int(rank_all1[0][idx])\n",
    "    test_data[key]['market1501_rank'] = int(rank_all1[1][idx])\n",
    "    test_data[key]['dukemtmc_rank'] = int(rank_all1[2][idx])\n",
    "    test_data[key]['msmt17_rank'] = int(rank_all1[3][idx])\n",
    "    test_data[key]['veri_rank'] = int(rank_all1[4][idx])\n",
    "    test_data[key]['vehicleid_rank'] = int(rank_all1[5][idx])\n",
    "    test_data[key]['veriwild_rank'] = int(rank_all1[6][idx])\n",
    "    test_data[key]['sop_rank'] = int(rank_all1[7][idx])\n",
    "\n",
    "print('Ready to save results!')\n",
    "with open('./CVPR_2022_NAS_Track2_submit_ACCNAS_1.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T14:29:21.952186Z",
     "iopub.status.busy": "2022-05-23T14:29:21.951591Z",
     "iopub.status.idle": "2022-05-23T22:30:43.024977Z",
     "shell.execute_reply": "2022-05-23T22:30:43.023585Z",
     "shell.execute_reply.started": "2022-05-23T14:29:21.952157Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:  0\n",
      "Variance 3.3439391282630595\n",
      "[[42459.]]\n",
      "No:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance 6.0485427006730506\n",
      "[[28879.]]\n",
      "No:  2\n",
      "Variance 6.006150913480946\n",
      "[[47402.]]\n",
      "No:  3\n",
      "Variance 6.103473245508546\n",
      "[[72536.]]\n",
      "No:  4\n",
      "Variance 6.025486965953786\n",
      "[[92206.]]\n",
      "No:  5\n",
      "Variance 5.8364155067895815\n",
      "[[70782.]]\n",
      "No:  6\n",
      "Variance 6.52441906729945\n",
      "[[92304.]]\n",
      "No:  7\n",
      "Variance 5.725986746928656\n",
      "[[90033.]]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Result 2, use GPNAS with identity matrix as initial covariance prior to stack 'GBRT','HISTGB','CATGB','LIGHTGB','XGB','GBRT2','CATGB2' with cross validation\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "X_train_k = np.array(arch_list_train)\n",
    "X_val = np.array(test_arch_list)\n",
    "\n",
    "rank_all2= []\n",
    "\n",
    "for i in range(len(list_est)):\n",
    "    print('No: ',i)\n",
    "    #stack different regressor by GPNAS with cross validation\n",
    "    model_final = StackingRegressor(estimators=list_est[i],final_estimator=GPNAS_API(c_flag=2, m_flag=2, hp_mat = 0.5, hp_cov = 0.01, icov = 0),passthrough=False,n_jobs=4)\n",
    "    Y_train_k = Y_all[i]\n",
    "    model_final.fit(X_train_k,Y_train_k)\n",
    "    zz = np.round((X_val.shape[0]-1)/(1+ np.exp(-1*model_final.predict(X_val)))) #Transfer by Sigmoid function\n",
    "    print(zz[0])\n",
    "    rank_all2.append(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:15:32.417739Z",
     "iopub.status.busy": "2022-05-24T00:15:32.416959Z",
     "iopub.status.idle": "2022-05-24T00:15:36.112842Z",
     "shell.execute_reply": "2022-05-24T00:15:36.112112Z",
     "shell.execute_reply.started": "2022-05-24T00:15:32.417705Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to save results!\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Save Result 2(My result is ran locally which is a little different from here)\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for idx,key in enumerate(test_data.keys()):\n",
    "    #print(key)\n",
    "    test_data[key]['cplfw_rank'] = int(rank_all2[0][idx])\n",
    "    test_data[key]['market1501_rank'] = int(rank_all2[1][idx])\n",
    "    test_data[key]['dukemtmc_rank'] = int(rank_all2[2][idx])\n",
    "    test_data[key]['msmt17_rank'] = int(rank_all2[3][idx])\n",
    "    test_data[key]['veri_rank'] = int(rank_all2[4][idx])\n",
    "    test_data[key]['vehicleid_rank'] = int(rank_all2[5][idx])\n",
    "    test_data[key]['veriwild_rank'] = int(rank_all2[6][idx])\n",
    "    test_data[key]['sop_rank'] = int(rank_all2[7][idx])\n",
    "\n",
    "print('Ready to save results!')\n",
    "with open('./CVPR_2022_NAS_Track2_submit_ACCNAS_2.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-22T14:29:30.161767Z",
     "iopub.status.idle": "2022-05-22T14:29:30.162093Z",
     "shell.execute_reply": "2022-05-22T14:29:30.161959Z",
     "shell.execute_reply.started": "2022-05-22T14:29:30.161918Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Multivariate Gradient Boosting (result is not that good final score 0.78759) \n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "#import numpy as np\n",
    "#train_num = 400;gb_list = [];rank_list= []\n",
    "#index_list = [[0],[1],[2],[3],[4],[5],[6],[7]]\n",
    "#index_list = [[0],[0,1],[0,2],[0,3],[0,4],[0,5],[0,6],[0,7]]#[50, 17, 43, 18, 3, 10, 28, 31]\n",
    "#index_list = [[1,0],[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],[1,7]]#1,1\n",
    "#index_list = [[1],[1,2],[1,3],[1,4],[1,2],[1,2,3],[1,2,3,4],[1,2,3,4,5]]#[48, 19, 36, 30, 0, 22, 23, 22]\n",
    "#index_list = [[2,0],[2,1],[2],[2,3],[2,4],[2,5],[2,6],[2,7]]#[1, 30, 68, 35, 25, 15, 14, 12]\n",
    "#index_list = [[3,0],[3,1],[3,2],[3,3],[3,4],[3,5],[3,6],[3,7]]#3,3\n",
    "#index_list = [[4,0],[4,1],[4,2],[4,3],[4,4],[4,5],[4,6],[4,7]]#[0, 25, 27, 31, 70, 8, 23, 16]\n",
    "#index_list = [[5,0],[5,1],[5,2],[5,3],[5,4],[5,5],[5,6],[5,7]]#[7, 18, 27, 22, 21, 56, 33, 16]\n",
    "#index_list = [[6,0],[6,1],[6,2],[6,3],[6,4],[6,5],[6],[6,7]]#[0, 25, 19, 29, 19, 24, 79, 5]\n",
    "#index_list = [[7,0],[7,1],[7,2],[7,3],[7,4],[7,5],[7,6],[7]]#[3, 14, 23, 20, 28, 19, 11, 82]\n",
    "#xx = np.zeros([len(index_list),50])\n",
    "#for j in range(xx.shape[1]):\n",
    "#  for i in range(xx.shape[0]):\n",
    "#    md_catboost = CatBoostRegressor(iterations=10000,\n",
    "#                             learning_rate=.005,\n",
    "#                             depth=2,\n",
    "#                             verbose=0,\n",
    "#                             #silent=True,\n",
    "#                             task_type=\"CPU\",\n",
    "#                             l2_leaf_reg=1,\n",
    "#                             loss_function= 'MultiRMSE',\n",
    "#                             eval_metric= 'MultiRMSE',\n",
    "#                             random_seed = 1,\n",
    "#                             bagging_temperature = 0.1,\n",
    "#                             od_type= 'Iter', \n",
    "#                             metric_period = 75,\n",
    "#                             od_wait=100)\n",
    "#    X_all_k, Y_all_k  = np.array(arch_list_train).astype('float'), Y_all.astype('float').T[:,index_list[i]]\n",
    "#    X_train_k, X_test_k, Y_train_k, Y_test_k = sklearn.model_selection.train_test_split(\n",
    "#        X_all_k, Y_all_k, test_size=0.2, shuffle=True,random_state=j)\n",
    "#\n",
    "#    md_catboost.fit(X_train_k,Y_train_k)\n",
    "#    y_predict = md_catboost.predict(X_test_k)\n",
    "#    if len(index_list[i]) == 1:\n",
    "#        mse = sklearn.metrics.mean_squared_error(y_predict,Y_test_k)\n",
    "#    else:\n",
    "#        mse = sklearn.metrics.mean_squared_error(y_predict[:,0],Y_test_k[:,0])\n",
    "#    print('MSE:','i',i,'j',j,index_list[i],mse)\n",
    "#    print('Kendalltau:',scipy.stats.stats.kendalltau(y_predict[:,0],Y_test_k[:,0]))]\n",
    "#    xx[i,j] = np.round(mse,5)\n",
    "\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "#X_train_k, Y_train_k = X_all_k, Y_all_k\n",
    "#print(X_train_k.shape, Y_train_k.shape, X_test_k.shape, Y_test_k.shape)\n",
    "#dtrain = Pool(X_train_k, label=Y_train_k)\n",
    "#dvalid = Pool(X_test_k, label=Y_test_k)\n",
    "#md_catboost.fit(dtrain,eval_set=dvalid, use_best_model=True,early_stopping_rounds=None)\n",
    "#y_predict = md_catboost.predict(dvalid)\n",
    "#[print('Kendalltau:',scipy.stats.stats.kendalltau(y_predict,Y_test_k))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-22T14:29:30.163331Z",
     "iopub.status.idle": "2022-05-22T14:29:30.163591Z",
     "shell.execute_reply": "2022-05-22T14:29:30.163470Z",
     "shell.execute_reply.started": "2022-05-22T14:29:30.163457Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Other experiment we try\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#Only CAT regressor\n",
    "#\tKendalltau:0.79394\t\t\n",
    "#Only HIST regressor\n",
    "#\tKendalltau:0.78626\t\n",
    "#Only GB regressor\n",
    "#\tKendalltau:0.79383\t\t\n",
    "\n",
    "#Only XGB regressor \n",
    "#\tKendalltau:0.78741\t\n",
    "#Only LIGHTGB regressor\n",
    "#\tKendalltau:0.78647\t\t\n",
    "\n",
    "#5 regressor combine with same learning rate and same depth parameters(depth = 1)\n",
    "#\tKendalltau:0.79321\t\t\n",
    "\n",
    "#5 regressor combine n_iters = 5000 with different learning rate for different regressor\n",
    "#\tKendalltau:0.79457\t\n",
    "\n",
    "#5 regressor combine n_iters = 10000\n",
    "#\tKendalltau:0.79696\t\n",
    "        \n",
    "#7 regressor combine, hp_mat = 1, learn_rate*0.8\n",
    "#\tKendalltau:0.79608\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 1\n",
    "#\tKendalltau:0.79697\t\n",
    "        \n",
    "#7 regressor combine, hp_mat = 1, learn_rate*2\n",
    "#\tKendalltau:0.79732\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 1, tunning learn_rate\n",
    "#\tKendalltau:0.79769\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.4\n",
    "#\tKendalltau:0.79785\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, all depth parameter equal to 1 \n",
    "#\tKendalltau:0.79389\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, tunning depth parameter\n",
    "#\tKendalltau:0.79788\t  \n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, round up final int rank\n",
    "#\tKendalltau:0.79796\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, tunning learn_rate after new depth parameter\n",
    "#\tKendalltau:0.79859\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
