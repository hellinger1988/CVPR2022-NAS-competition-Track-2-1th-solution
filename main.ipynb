{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T14:15:08.514511Z",
     "iopub.status.busy": "2022-05-21T14:15:08.513276Z",
     "iopub.status.idle": "2022-05-21T14:15:21.950058Z",
     "shell.execute_reply": "2022-05-21T14:15:21.949373Z",
     "shell.execute_reply.started": "2022-05-21T14:15:08.514462Z"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Our 2022 CVPR Track2 Solution \n",
    "\n",
    "Accurately Predicting the performance of architecture with small sample training is a very important but not easy task. How to analysis and train data as much as we can and overcome over fitting is the core problem we should deal with. Meanwhile if there is the mult-task problem, we should also think about if we can take advantage of their correlation. \n",
    "\n",
    "In this track Super Network builds a search space based on ViT-Base.The search space contain depth, num_heads, mpl_ratio and embed_dim. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:20:11.720442Z",
     "iopub.status.busy": "2022-05-26T05:20:11.719968Z",
     "iopub.status.idle": "2022-05-26T05:24:59.094690Z",
     "shell.execute_reply": "2022-05-26T05:24:59.094010Z",
     "shell.execute_reply.started": "2022-05-26T05:20:11.720401Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Install Packages\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "#!mkdir /home/aistudio/external-libraries\n",
    "#!pip install --upgrade sklearn -i https://mirrors.aliyun.com/pypi/simple/ -t /home/aistudio/externallibraries\n",
    "#!conda install lightgbm \n",
    "#!conda install xgboost -i https://mirrors.aliyun.com/pypi/simple/\n",
    "!pip install catboost -i https://mirrors.aliyun.com/pypi/simple/\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Import Packages\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import catboost\n",
    "import lightgbm\n",
    "import xgboost\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import *#HistGradientBoostingRegressor,StackingRegressor,BaggingRegressor,ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import *\n",
    "from sklearn.linear_model import *#LinearRegression\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import scipy\n",
    "import copy\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.linalg import hankel\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:34:36.726081Z",
     "iopub.status.busy": "2022-05-26T05:34:36.725492Z",
     "iopub.status.idle": "2022-05-26T05:34:39.223755Z",
     "shell.execute_reply": "2022-05-26T05:34:39.222980Z",
     "shell.execute_reply.started": "2022-05-26T05:34:36.726037Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Load trainning and Testing data\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "def convert_X(arch_str):\n",
    "    temp_arch = []\n",
    "    for i,elm in enumerate(arch_str):\n",
    "        if i in [3,6,9,12,15,18,21,24,27,30,33,36]: pass #Get rid of non-info columns,all is 768\n",
    "        elif elm == 'j': temp_arch.append(1-2) #Transform it to number then central data, normalize Data\n",
    "        elif elm == 'k': temp_arch.append(2-2) #Transform it to number then central data, normalize Data\n",
    "        elif elm == 'l': temp_arch.append(3-2)  #Transform it to number then central data, normalize Data\n",
    "        elif int(elm) == 0: temp_arch.append(2-2)  #Make 0 as 2 as it should contain neutral information（reduce correlation), then central data, normalize Data\n",
    "        else: temp_arch.append(int(elm)-2)  #central data,normalize Data\n",
    "    return(temp_arch)\n",
    "with open('./data/data134077/CVPR_2022_NAS_Track2_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('./data/data134077/CVPR_2022_NAS_Track2_test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "    \n",
    "test_arch_list = []\n",
    "for key in test_data.keys():\n",
    "    test_arch =  convert_X(test_data[key]['arch'])\n",
    "    test_arch_list.append(test_arch)\n",
    "bb = np.array(test_arch_list).T\n",
    "\n",
    "train_list = [[],[],[],[],[],[],[],[]]\n",
    "arch_list_train = []\n",
    "name_list = ['cplfw_rank', 'market1501_rank', 'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank', 'veriwild_rank', 'sop_rank']\n",
    "for key in train_data.keys():\n",
    "    for idx, name in enumerate(name_list):\n",
    "        train_list[idx].append(train_data[key][name])\n",
    "    xx = train_data[key]['arch']\n",
    "    arch_list_train.append(convert_X(train_data[key]['arch']))\n",
    "\n",
    "Y_all0 = np.array(train_list)\n",
    "Y_all = np.log((Y_all0+1)/(500-Y_all0)) #Transfer rank data by Sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T05:34:39.237571Z",
     "iopub.status.busy": "2022-05-26T05:34:39.237235Z",
     "iopub.status.idle": "2022-05-26T05:34:39.349132Z",
     "shell.execute_reply": "2022-05-26T05:34:39.348498Z",
     "shell.execute_reply.started": "2022-05-26T05:34:39.237548Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#Correlation reserach \n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "regcoef = np.round(np.array([LinearRegression().fit(np.array(arch_list_train), Y_all[i]).coef_ for i in range(8)]),2)\n",
    "print('Task Corr: ')\n",
    "print(np.round(np.corrcoef(regcoef),2))#correlation bewtween different task\n",
    "print('Parameters Corr: ')\n",
    "print(np.round(np.corrcoef(np.array(arch_list_train).T.astype('float')),2))\n",
    "#correlation bewtween different task, we can find the correlation is very low between different arch parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:17:40.796976Z",
     "iopub.status.busy": "2022-05-24T00:17:40.796710Z",
     "iopub.status.idle": "2022-05-24T00:17:40.822977Z",
     "shell.execute_reply": "2022-05-24T00:17:40.822423Z",
     "shell.execute_reply.started": "2022-05-24T00:17:40.796950Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Modify GPNAS code as sklearn API and add some more function\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "__all__ = [\"GPNAS_API\"]\n",
    "\n",
    "class GPNAS_API(object):\n",
    "    _estimator_type = \"regressor\"\n",
    "    def __init__(self, cov_w = None, w = None, c_flag=2, m_flag=2, hp_mat = 0.0000001, hp_cov = 0.01, icov = 1):\n",
    "        self.hp_mat = hp_mat\n",
    "        self.hp_cov = hp_cov\n",
    "        self.cov_w = cov_w\n",
    "        self.w = w \n",
    "        self.c_flag = c_flag\n",
    "        self.m_flag = m_flag\n",
    "        self.icov = icov #if we use initial cov as prior\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"hp_mat\": self.hp_mat, \n",
    "            \"hp_cov\": self.hp_cov, \n",
    "            \"cov_w\": self.cov_w, \n",
    "            \"w\": self.w, \n",
    "            \"c_flag\": self.c_flag, \n",
    "            \"m_flag\": self.m_flag, \n",
    "            \"icov\": self.icov, \n",
    "            }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def _get_corelation(self, mat1, mat2):\n",
    "        \"\"\"\n",
    "        give two typical kernel function\n",
    "        Auto kernel hyperparameters estimation to be updated\n",
    "        \"\"\"\n",
    "\n",
    "        mat_diff = abs(mat1 - mat2)\n",
    "\n",
    "        if self.c_flag == 1:\n",
    "\n",
    "            return 0.5 * np.exp(-np.dot(mat_diff, mat_diff) / 16)\n",
    "\n",
    "        elif self.c_flag == 2:\n",
    "\n",
    "            return 1 * np.exp(-np.sqrt(np.dot(mat_diff, mat_diff)) / 12)\n",
    "\n",
    "    def _preprocess_X(self, X):\n",
    "        \"\"\"\n",
    "        preprocess of input feature/ tokens of architecture\n",
    "        more complicated preprocess can be added such as nonlineaer transformation\n",
    "        \"\"\"\n",
    "        X = X.tolist()\n",
    "        p_X = copy.deepcopy(X)\n",
    "        for feature in p_X: feature.append(1)\n",
    "        return p_X\n",
    "\n",
    "    def _get_cor_mat(self, X):\n",
    "        \"\"\"get kernel matrix\"\"\"\n",
    "        X = np.array(X)\n",
    "        l = X.shape[0]\n",
    "        cor_mat = []\n",
    "\n",
    "        for c_idx in range(l):\n",
    "            col = []\n",
    "            c_mat = X[c_idx].copy()\n",
    "\n",
    "            for r_idx in range(l):\n",
    "                r_mat = X[r_idx].copy()\n",
    "                temp_cor = self._get_corelation(c_mat, r_mat)\n",
    "                col.append(temp_cor)\n",
    "            cor_mat.append(col)\n",
    "\n",
    "        return np.mat(cor_mat)\n",
    "\n",
    "    def _get_cor_mat_joint(self, X, X_train):\n",
    "        \"\"\"\n",
    "        get kernel matrix\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        X_train = np.array(X_train)\n",
    "        l_c = X.shape[0]\n",
    "        l_r = X_train.shape[0]\n",
    "        cor_mat = []\n",
    "\n",
    "        for c_idx in range(l_c):\n",
    "            col = []\n",
    "            c_mat = X[c_idx].copy()\n",
    "\n",
    "            for r_idx in range(l_r):\n",
    "                r_mat = X_train[r_idx].copy()\n",
    "                temp_cor = self._get_corelation(c_mat, r_mat)\n",
    "                col.append(temp_cor)\n",
    "            cor_mat.append(col)\n",
    "\n",
    "        return np.mat(cor_mat)\n",
    "\n",
    "\n",
    "    def fit(self, X,y):\n",
    "        self.get_initial_mean(X[0::2],y[0::2])\n",
    "        self.get_initial_cov(X)\n",
    "        # 更新（训练）gpnas预测器超参数\n",
    "        self.get_posterior_mean(X[1::2],y[1::2])\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        #print('beta',self.w.flatten())\n",
    "        return X * self.w\n",
    "            \n",
    "    def get_predict(self, X):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "\n",
    "        return X * self.w\n",
    "\n",
    "    def get_predict_jiont(self, X, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X based on X_train and Y_train\n",
    "        \"\"\"\n",
    "        X = np.mat(X)\n",
    "        X_train = np.mat(X_train)\n",
    "        Y_train = np.mat(Y_train)\n",
    "        m_X = self.get_predict(X)\n",
    "        m_X_train = self.get_predict(X_train)\n",
    "        mat_train = self._get_cor_mat(X_train)\n",
    "        mat_joint = self._get_cor_mat_joint(X, X_train)\n",
    "\n",
    "        return m_X + mat_joint * np.linalg.inv(mat_train + self.hp_mat * np.eye(\n",
    "            X_train.shape[0])) * (Y_train.T - m_X_train)\n",
    "\n",
    "    def get_initial_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get initial mean of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        self.w = np.linalg.inv(X.T * X + self.hp_mat * np.eye(X.shape[\n",
    "            1])) * X.T * Y.T\n",
    "        #inv(X.T*X)X.T*Y as initial mean\n",
    "        print('Variance',np.var(Y-X * self.w))#Show variance of residual then we can base this tunning self.hp_cov\n",
    "        return self.w\n",
    "\n",
    "    def get_initial_cov(self, X):\n",
    "        \"\"\"\n",
    "        get initial coviarnce matrix of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        if self.icov == 1: #use inv(X.T*X) as initial covariance\n",
    "            self.cov_w = self.hp_cov * np.linalg.inv(X.T * X)\n",
    "        elif self.icov == 0:# use identity matrix as initial covariance\n",
    "            self.cov_w = self.hp_cov * np.eye(X.shape[1])\n",
    "        else:\n",
    "            assert 0,'not available yet'\n",
    "        return self.cov_w\n",
    "\n",
    "    def get_posterior_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior mean of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        cov_mat = self._get_cor_mat(X)\n",
    "        if self.m_flag == 1:\n",
    "            self.w = self.w + self.cov_w * X.T * np.linalg.inv(\n",
    "                np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0])) + X *\n",
    "                self.cov_w * X.T + self.hp_mat * np.eye(X.shape[0])) * (\n",
    "                    Y.T - X * self.w)\n",
    "        else:\n",
    "            self.w = np.linalg.inv(X.T * np.linalg.inv(\n",
    "                cov_mat + self.hp_mat * np.eye(X.shape[0])) * X + np.linalg.inv(\n",
    "                    self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                        1])) + self.hp_mat * np.eye(X.shape[1])) * (\n",
    "                            X.T * np.linalg.inv(cov_mat + self.hp_mat * np.eye(\n",
    "                                X.shape[0])) * Y.T +\n",
    "                            np.linalg.inv(self.cov_w + self.hp_mat * np.eye(\n",
    "                                X.shape[1])) * self.w)\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def get_posterior_cov(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior coviarnce matrix of w\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        cov_mat = self._get_cor_mat(X)\n",
    "        self.cov_mat = np.linalg.inv(\n",
    "            np.linalg.inv(X.T * cov_mat * X + self.hp_mat * np.eye(X.shape[1]))\n",
    "            + np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                1])) + self.hp_mat * np.eye(X.shape[1]))\n",
    "        return self.cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:17:42.370303Z",
     "iopub.status.busy": "2022-05-24T00:17:42.370035Z",
     "iopub.status.idle": "2022-05-24T00:17:42.384130Z",
     "shell.execute_reply": "2022-05-24T00:17:42.383589Z",
     "shell.execute_reply.started": "2022-05-24T00:17:42.370278Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Modify GPNAS code as sklearn API and add some more function\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "max_iter = [10000,10000,10000,10000,10000,10000,10000,10000] \n",
    "\n",
    "#learning_rate = [0.008,0.038,0.032,0.02,0.025,0.012,0.025,0.006]\n",
    "#learning_rate = [0.01.,0.04.,0.04.,0.04.,0.02.,0.02.,0.04.,0.001.]\n",
    "learning_rate = [0.005,0.038,0.035,0.03,0.025,0.01,0.03,0.01] #Final learning rate\n",
    "max_depth = [1,3,2,2,2,3,1,3] #depth for GBRT(huber),CATGB(MSE),GBRT2(MSE),CATGB2(huber)\n",
    "max_depth2 = [1,1,1,1,1,1,1,1] #depth for HISTGB,LIGHTGB,XGB\n",
    "list_est = []\n",
    "\n",
    "model_GBRT,model_HISTGB,model_CATGB,model_LIGHTGB,model_XGB,model_GBRT2,model_CATGB2= [],[],[],[],[],[],[]\n",
    "for i in range(8):\n",
    "\n",
    "    params_GBRT = {\"n_estimators\": max_iter[i],\n",
    "    \"max_depth\": max_depth[i],\n",
    "    \"subsample\": .8,\n",
    "    \"learning_rate\": learning_rate[i],\n",
    "    \"loss\": 'huber',\n",
    "    \"max_features\": 'sqrt',\n",
    "    \"random_state\":1,\n",
    "    } \n",
    "    model_GBRT.append(ensemble.GradientBoostingRegressor(**params_GBRT)) \n",
    "    \n",
    "    params_HISTGB = {\n",
    "    \"max_depth\": max_depth2[i],\n",
    "    \"max_iter\":max_iter[i] ,\n",
    "    \"learning_rate\": learning_rate[i],\n",
    "    \"loss\": 'least_squares',\n",
    "    \"max_leaf_nodes\":31,\n",
    "    \"min_samples_leaf\":5,\n",
    "    \"l2_regularization\":5,\n",
    "    \"random_state\":1,\n",
    "    }\n",
    "    model_HISTGB.append(HistGradientBoostingRegressor(**params_HISTGB))\n",
    "\n",
    "\n",
    "    model_CATGB.append(catboost.CatBoostRegressor(iterations= max_iter[i] ,\n",
    "                             learning_rate= learning_rate[i],\n",
    "                             depth= max_depth[i],\n",
    "                             silent=True,\n",
    "                             task_type=\"CPU\",\n",
    "                             loss_function= 'RMSE',                     \n",
    "                             eval_metric='RMSE',\n",
    "                             random_seed = 1,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 75,\n",
    "                             od_wait=100,\n",
    "                             ))\n",
    "\n",
    "    \n",
    "    model_LIGHTGB.append(lightgbm.LGBMRegressor(boosting_type='gbdt',learning_rate = learning_rate[i],num_leaves=31,\n",
    "    max_depth = max_depth2[i], alpha = 0.1, n_estimators = max_iter[i] ,random_state=1))\n",
    "    \n",
    "    model_XGB.append(xgboost.XGBRegressor(learning_rate = learning_rate[i],tree_method = 'auto',\n",
    "    max_depth = max_depth2[i], alpha = 0.8, n_estimators = max_iter[i] ,random_state=1))   \n",
    "\n",
    "    params_GBRT2 = {\"n_estimators\": max_iter[i] ,\n",
    "    \"max_depth\": max_depth[i],\n",
    "    \"subsample\": .8,\n",
    "    \"learning_rate\": learning_rate[i],\n",
    "    \"loss\": 'ls',\n",
    "    \"max_features\": 'log2',\n",
    "    \"random_state\":1,\n",
    "    } \n",
    "    model_GBRT2.append(ensemble.GradientBoostingRegressor(**params_GBRT2)) \n",
    "    \n",
    "    model_CATGB2.append(catboost.CatBoostRegressor(iterations= max_iter[i] ,\n",
    "                             learning_rate= learning_rate[i],\n",
    "                             depth= max_depth[i],\n",
    "                             silent=True,\n",
    "                             task_type=\"CPU\",\n",
    "                             loss_function=  'Huber:delta=2',                     \n",
    "                             eval_metric= 'Huber:delta=2',\n",
    "                             random_seed = 1,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 75,\n",
    "                             od_wait=100,\n",
    "                             l2_leaf_reg = 1,\n",
    "                             subsample = 0.8,\n",
    "                             ))\n",
    "\n",
    "for i in range(8): \n",
    "    list_est.append([\n",
    "    ('GBRT', model_GBRT[i]),\n",
    "    ('HISTGB', model_HISTGB[i]),\n",
    "    ('CATGB',model_CATGB[i]),\n",
    "    ('LIGHTGB', model_LIGHTGB[i]),\n",
    "    ('XGB', model_XGB[i]),\n",
    "    ('GBRT2', model_GBRT2[i]),\n",
    "    ('CATGB2', model_CATGB2[i]),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:01.070177Z",
     "iopub.status.busy": "2022-05-23T13:13:01.069587Z",
     "iopub.status.idle": "2022-05-23T13:13:01.073095Z",
     "shell.execute_reply": "2022-05-23T13:13:01.072552Z",
     "shell.execute_reply.started": "2022-05-23T13:13:01.070147Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    In sample training and testing, just for reserach and parameter selection\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#X_all_k = np.array(arch_list_train)\n",
    "#X_val = np.array(test_arch_list)\n",
    "#print(np.array(test_arch_list).shape,X_val.shape)\n",
    "#train_num = 400;\n",
    "#gb_list = []\n",
    "#for i in range(8):\n",
    "#    model_final = StackingRegressor(estimators=list_est[i],final_estimator=GPNAS_API(c_flag=2, m_flag=2, hp_mat = hp_list[i], hp_cov = 3, icov = 1),passthrough=False,n_jobs=4)\n",
    "#    Y_all_k = Y_all[i]\n",
    "#    X_train_k, Y_train_k, X_test_k, Y_test_k = X_all_k[0:train_num:1], Y_all_k[0:train_num:1], X_all_k[train_num::1], Y_all_k[train_num::1]\n",
    "#    model_final.fit(X_train_k,Y_train_k)\n",
    "#    gb_list.append(copy.copy(model_final))\n",
    "#    print('Kendalltau:',i,scipy.stats.stats.kendalltau(model_final.predict(X_test_k),Y_test_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:03.412908Z",
     "iopub.status.busy": "2022-05-23T13:13:03.412270Z",
     "iopub.status.idle": "2022-05-23T13:13:03.416007Z",
     "shell.execute_reply": "2022-05-23T13:13:03.415503Z",
     "shell.execute_reply.started": "2022-05-23T13:13:03.412880Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Plot training and testing params result to help make prediction.\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#i = 0\n",
    "#params = gb_list[i].get_params()\n",
    "#itr = \"n_estimators\"\n",
    "#test_score0 = np.zeros((params[itr],), dtype=np.float64)\n",
    "#test_score1 = np.zeros((params[itr],), dtype=np.float64)\n",
    "\n",
    "#for i,y_pred0 in enumerate(model_gb.staged_predict(X_train_k)):\n",
    "#    test_score0[i] = scipy.stats.stats.kendalltau(Y_train_k, y_pred0)[0]\n",
    "#for i,y_pred1 in enumerate(model_gb.staged_predict(X_test_k)):\n",
    "#    test_score1[i] = scipy.stats.stats.kendalltau(Y_test_k, y_pred1)[0]\n",
    "    \n",
    "#fig = plt.figure(figsize=(6, 6))\n",
    "#plt.subplot(1, 1, 1)\n",
    "#plt.title(\"Deviance\")\n",
    "#plt.plot(\n",
    "#    np.arange(params[itr])[:] + 1,\n",
    "#    test_score0[:],\n",
    "#    \"b-\",\n",
    "#    label=\"Training Set Deviance\",\n",
    "#)\n",
    "#plt.plot(np.arange(params[itr])[:] + 1, test_score1[:], \"r-\", label=\"Test Set Deviance\")\n",
    "#plt.legend(loc=\"upper right\")\n",
    "#plt.xlabel(\"Boosting Iterations\")\n",
    "#plt.ylabel(\"Deviance\")\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:05.791280Z",
     "iopub.status.busy": "2022-05-23T13:13:05.790682Z",
     "iopub.status.idle": "2022-05-23T13:13:05.794026Z",
     "shell.execute_reply": "2022-05-23T13:13:05.793500Z",
     "shell.execute_reply.started": "2022-05-23T13:13:05.791254Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#   Feature Importance\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#from sklearn.inspection import permutation_importance\n",
    "#for model_gb in gb_list:\n",
    "#  feature_importance = model_gb.feature_importances_\n",
    "#  sorted_idx = np.argsort(feature_importance)\n",
    "#  pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "#  fig = plt.figure(figsize=(6, 4))\n",
    "#  plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "#  plt.yticks(pos, np.array(range(len(feature_importance)))[sorted_idx])\n",
    "#  plt.title(\"Feature Importance \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T13:13:47.011185Z",
     "iopub.status.busy": "2022-05-23T13:13:47.010571Z",
     "iopub.status.idle": "2022-05-23T14:29:06.242003Z",
     "shell.execute_reply": "2022-05-23T14:29:06.240534Z",
     "shell.execute_reply.started": "2022-05-23T13:13:47.011158Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Result 1, use GPNAS with inv(X.T*X) as initial covariance prior to stack 'GBRT','HISTGB','CATGB','LIGHTGB','XGB','GBRT2','CATGB2' with cross validation\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "X_train_k = np.array(arch_list_train)\n",
    "X_val = np.array(test_arch_list)\n",
    "\n",
    "rank_all1= []\n",
    "for i in range(len(list_est)):\n",
    "    print('No: ',i)\n",
    "    #stack different regressor by GPNAS with cross validation\n",
    "    model_final = StackingRegressor(estimators=list_est[i],final_estimator=GPNAS_API(c_flag=2, m_flag=2, hp_mat = 0.5, hp_cov = 3, icov = 1),passthrough=False,n_jobs=4)\n",
    "    Y_train_k = Y_all[i]\n",
    "    model_final.fit(X_train_k,Y_train_k)\n",
    "    zz = np.round((X_val.shape[0]-1)/(1+ np.exp(-1*model_final.predict(X_val)))) #Transfer by Sigmoid function\n",
    "    print(zz[0])\n",
    "    rank_all1.append(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-22T14:29:30.156108Z",
     "iopub.status.idle": "2022-05-22T14:29:30.156419Z",
     "shell.execute_reply": "2022-05-22T14:29:30.156270Z",
     "shell.execute_reply.started": "2022-05-22T14:29:30.156256Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Save Result 1(My result is ran locally which is a little different from here)\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "for idx,key in enumerate(test_data.keys()):\n",
    "    #print(key)\n",
    "    test_data[key]['cplfw_rank'] = int(rank_all1[0][idx])\n",
    "    test_data[key]['market1501_rank'] = int(rank_all1[1][idx])\n",
    "    test_data[key]['dukemtmc_rank'] = int(rank_all1[2][idx])\n",
    "    test_data[key]['msmt17_rank'] = int(rank_all1[3][idx])\n",
    "    test_data[key]['veri_rank'] = int(rank_all1[4][idx])\n",
    "    test_data[key]['vehicleid_rank'] = int(rank_all1[5][idx])\n",
    "    test_data[key]['veriwild_rank'] = int(rank_all1[6][idx])\n",
    "    test_data[key]['sop_rank'] = int(rank_all1[7][idx])\n",
    "\n",
    "print('Ready to save results!')\n",
    "with open('./CVPR_2022_NAS_Track2_submit_ACCNAS_1.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T14:29:21.952186Z",
     "iopub.status.busy": "2022-05-23T14:29:21.951591Z",
     "iopub.status.idle": "2022-05-23T22:30:43.024977Z",
     "shell.execute_reply": "2022-05-23T22:30:43.023585Z",
     "shell.execute_reply.started": "2022-05-23T14:29:21.952157Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Result 2, use GPNAS with identity matrix as initial covariance prior to stack 'GBRT','HISTGB','CATGB','LIGHTGB','XGB','GBRT2','CATGB2' with cross validation\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "X_train_k = np.array(arch_list_train)\n",
    "X_val = np.array(test_arch_list)\n",
    "\n",
    "rank_all2= []\n",
    "\n",
    "for i in range(len(list_est)):\n",
    "    print('No: ',i)\n",
    "    #stack different regressor by GPNAS with cross validation\n",
    "    model_final = StackingRegressor(estimators=list_est[i],final_estimator=GPNAS_API(c_flag=2, m_flag=2, hp_mat = 0.5, hp_cov = 0.01, icov = 0),passthrough=False,n_jobs=4)\n",
    "    Y_train_k = Y_all[i]\n",
    "    model_final.fit(X_train_k,Y_train_k)\n",
    "    zz = np.round((X_val.shape[0]-1)/(1+ np.exp(-1*model_final.predict(X_val)))) #Transfer by Sigmoid function\n",
    "    print(zz[0])\n",
    "    rank_all2.append(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T00:15:32.417739Z",
     "iopub.status.busy": "2022-05-24T00:15:32.416959Z",
     "iopub.status.idle": "2022-05-24T00:15:36.112842Z",
     "shell.execute_reply": "2022-05-24T00:15:36.112112Z",
     "shell.execute_reply.started": "2022-05-24T00:15:32.417705Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Save Result 2(My result is ran locally which is a little different from here)\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for idx,key in enumerate(test_data.keys()):\n",
    "    #print(key)\n",
    "    test_data[key]['cplfw_rank'] = int(rank_all2[0][idx])\n",
    "    test_data[key]['market1501_rank'] = int(rank_all2[1][idx])\n",
    "    test_data[key]['dukemtmc_rank'] = int(rank_all2[2][idx])\n",
    "    test_data[key]['msmt17_rank'] = int(rank_all2[3][idx])\n",
    "    test_data[key]['veri_rank'] = int(rank_all2[4][idx])\n",
    "    test_data[key]['vehicleid_rank'] = int(rank_all2[5][idx])\n",
    "    test_data[key]['veriwild_rank'] = int(rank_all2[6][idx])\n",
    "    test_data[key]['sop_rank'] = int(rank_all2[7][idx])\n",
    "\n",
    "print('Ready to save results!')\n",
    "with open('./CVPR_2022_NAS_Track2_submit_ACCNAS_2.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-22T14:29:30.161767Z",
     "iopub.status.idle": "2022-05-22T14:29:30.162093Z",
     "shell.execute_reply": "2022-05-22T14:29:30.161959Z",
     "shell.execute_reply.started": "2022-05-22T14:29:30.161918Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Multivariate Gradient Boosting (result is not that good final score 0.78759) \n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "#import numpy as np\n",
    "#train_num = 400;gb_list = [];rank_list= []\n",
    "#index_list = [[0],[1],[2],[3],[4],[5],[6],[7]]\n",
    "#index_list = [[0],[0,1],[0,2],[0,3],[0,4],[0,5],[0,6],[0,7]]#[50, 17, 43, 18, 3, 10, 28, 31]\n",
    "#index_list = [[1,0],[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],[1,7]]#1,1\n",
    "#index_list = [[1],[1,2],[1,3],[1,4],[1,2],[1,2,3],[1,2,3,4],[1,2,3,4,5]]#[48, 19, 36, 30, 0, 22, 23, 22]\n",
    "#index_list = [[2,0],[2,1],[2],[2,3],[2,4],[2,5],[2,6],[2,7]]#[1, 30, 68, 35, 25, 15, 14, 12]\n",
    "#index_list = [[3,0],[3,1],[3,2],[3,3],[3,4],[3,5],[3,6],[3,7]]#3,3\n",
    "#index_list = [[4,0],[4,1],[4,2],[4,3],[4,4],[4,5],[4,6],[4,7]]#[0, 25, 27, 31, 70, 8, 23, 16]\n",
    "#index_list = [[5,0],[5,1],[5,2],[5,3],[5,4],[5,5],[5,6],[5,7]]#[7, 18, 27, 22, 21, 56, 33, 16]\n",
    "#index_list = [[6,0],[6,1],[6,2],[6,3],[6,4],[6,5],[6],[6,7]]#[0, 25, 19, 29, 19, 24, 79, 5]\n",
    "#index_list = [[7,0],[7,1],[7,2],[7,3],[7,4],[7,5],[7,6],[7]]#[3, 14, 23, 20, 28, 19, 11, 82]\n",
    "#xx = np.zeros([len(index_list),50])\n",
    "#for j in range(xx.shape[1]):\n",
    "#  for i in range(xx.shape[0]):\n",
    "#    md_catboost = CatBoostRegressor(iterations=10000,\n",
    "#                             learning_rate=.005,\n",
    "#                             depth=2,\n",
    "#                             verbose=0,\n",
    "#                             #silent=True,\n",
    "#                             task_type=\"CPU\",\n",
    "#                             l2_leaf_reg=1,\n",
    "#                             loss_function= 'MultiRMSE',\n",
    "#                             eval_metric= 'MultiRMSE',\n",
    "#                             random_seed = 1,\n",
    "#                             bagging_temperature = 0.1,\n",
    "#                             od_type= 'Iter', \n",
    "#                             metric_period = 75,\n",
    "#                             od_wait=100)\n",
    "#    X_all_k, Y_all_k  = np.array(arch_list_train).astype('float'), Y_all.astype('float').T[:,index_list[i]]\n",
    "#    X_train_k, X_test_k, Y_train_k, Y_test_k = sklearn.model_selection.train_test_split(\n",
    "#        X_all_k, Y_all_k, test_size=0.2, shuffle=True,random_state=j)\n",
    "#\n",
    "#    md_catboost.fit(X_train_k,Y_train_k)\n",
    "#    y_predict = md_catboost.predict(X_test_k)\n",
    "#    if len(index_list[i]) == 1:\n",
    "#        mse = sklearn.metrics.mean_squared_error(y_predict,Y_test_k)\n",
    "#    else:\n",
    "#        mse = sklearn.metrics.mean_squared_error(y_predict[:,0],Y_test_k[:,0])\n",
    "#    print('MSE:','i',i,'j',j,index_list[i],mse)\n",
    "#    print('Kendalltau:',scipy.stats.stats.kendalltau(y_predict[:,0],Y_test_k[:,0]))]\n",
    "#    xx[i,j] = np.round(mse,5)\n",
    "\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "#X_train_k, Y_train_k = X_all_k, Y_all_k\n",
    "#print(X_train_k.shape, Y_train_k.shape, X_test_k.shape, Y_test_k.shape)\n",
    "#dtrain = Pool(X_train_k, label=Y_train_k)\n",
    "#dvalid = Pool(X_test_k, label=Y_test_k)\n",
    "#md_catboost.fit(dtrain,eval_set=dvalid, use_best_model=True,early_stopping_rounds=None)\n",
    "#y_predict = md_catboost.predict(dvalid)\n",
    "#[print('Kendalltau:',scipy.stats.stats.kendalltau(y_predict,Y_test_k))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-22T14:29:30.163331Z",
     "iopub.status.idle": "2022-05-22T14:29:30.163591Z",
     "shell.execute_reply": "2022-05-22T14:29:30.163470Z",
     "shell.execute_reply.started": "2022-05-22T14:29:30.163457Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#    Other experiment we try\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#Only CAT regressor\n",
    "#\tKendalltau:0.79394\t\t\n",
    "#Only HIST regressor\n",
    "#\tKendalltau:0.78626\t\n",
    "#Only GB regressor\n",
    "#\tKendalltau:0.79383\t\t\n",
    "\n",
    "#Only XGB regressor \n",
    "#\tKendalltau:0.78741\t\n",
    "#Only LIGHTGB regressor\n",
    "#\tKendalltau:0.78647\t\t\n",
    "\n",
    "#5 regressor combine with same learning rate and same depth parameters(depth = 1)\n",
    "#\tKendalltau:0.79321\t\t\n",
    "\n",
    "#5 regressor combine n_iters = 5000 with different learning rate for different regressor\n",
    "#\tKendalltau:0.79457\t\n",
    "\n",
    "#5 regressor combine n_iters = 10000\n",
    "#\tKendalltau:0.79696\t\n",
    "        \n",
    "#7 regressor combine, hp_mat = 1, learn_rate*0.8\n",
    "#\tKendalltau:0.79608\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 1\n",
    "#\tKendalltau:0.79697\t\n",
    "        \n",
    "#7 regressor combine, hp_mat = 1, learn_rate*2\n",
    "#\tKendalltau:0.79732\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 1, tunning learn_rate\n",
    "#\tKendalltau:0.79769\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.4\n",
    "#\tKendalltau:0.79785\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, all depth parameter equal to 1 \n",
    "#\tKendalltau:0.79389\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, tunning depth parameter\n",
    "#\tKendalltau:0.79788\t  \n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, round up final int rank\n",
    "#\tKendalltau:0.79796\t\n",
    "\n",
    "#7 regressor combine, hp_mat = 0.5, tunning learn_rate after new depth parameter\n",
    "#\tKendalltau:0.79859\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
